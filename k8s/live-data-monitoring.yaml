# Enhanced Expert LLM System with Live Data Monitoring
apiVersion: v1
kind: ConfigMap
metadata:
  name: live-data-config
  namespace: expert-llm-system
data:
  monitoring_script.py: |
    #!/usr/bin/env python3
    import time
    import json
    import random
    import requests
    from datetime import datetime
    import psutil
    import os

    def generate_live_system_data():
        """Generate realistic live system data for the Expert LLM System"""
        
        # System metrics
        system_data = {
            "timestamp": datetime.now().isoformat(),
            "cpu_usage": psutil.cpu_percent(interval=1),
            "memory_usage": psutil.virtual_memory().percent,
            "disk_usage": psutil.disk_usage('/').percent,
            "load_average": os.getloadavg()[0] if hasattr(os, 'getloadavg') else random.uniform(0.5, 2.0),
            "network_connections": len(psutil.net_connections()),
            "running_processes": len(psutil.pids())
        }
        
        # Simulate various system issues for pattern recognition
        issues = [
            {
                "type": "cpu_spike",
                "description": "High CPU usage detected on worker node",
                "severity": "warning" if system_data["cpu_usage"] > 70 else "info",
                "pattern_match": "ubuntu_performance_cpu_high",
                "confidence": 0.85 + random.uniform(-0.1, 0.1)
            },
            {
                "type": "memory_pressure",
                "description": "Memory usage approaching limits",
                "severity": "critical" if system_data["memory_usage"] > 85 else "warning",
                "pattern_match": "ubuntu_memory_oom_killer",
                "confidence": 0.78 + random.uniform(-0.15, 0.15)
            },
            {
                "type": "disk_space",
                "description": "Disk space monitoring alert",
                "severity": "critical" if system_data["disk_usage"] > 90 else "info",
                "pattern_match": "ubuntu_storage_disk_full",
                "confidence": 0.92 + random.uniform(-0.05, 0.05)
            }
        ]
        
        # Add Kubernetes-specific issues
        k8s_issues = [
            {
                "type": "pod_crash",
                "description": "Pod in CrashLoopBackOff state",
                "severity": "critical",
                "pattern_match": "k8s_pod_crashloop",
                "confidence": 0.88
            },
            {
                "type": "service_discovery",
                "description": "Service endpoint not ready",
                "severity": "warning",
                "pattern_match": "k8s_service_discovery_dns",
                "confidence": 0.75
            }
        ]
        
        # Add GlusterFS-specific issues
        gluster_issues = [
            {
                "type": "brick_offline",
                "description": "GlusterFS brick went offline",
                "severity": "critical",
                "pattern_match": "glusterfs_brick_offline",
                "confidence": 0.91
            },
            {
                "type": "volume_heal",
                "description": "GlusterFS volume needs healing",
                "severity": "warning",
                "pattern_match": "glusterfs_volume_heal_needed",
                "confidence": 0.82
            }
        ]
        
        # Combine all issues and select random ones
        all_issues = issues + k8s_issues + gluster_issues
        active_issues = random.sample(all_issues, k=random.randint(1, 3))
        
        return {
            "system_metrics": system_data,
            "active_issues": active_issues,
            "expert_patterns_matched": len(active_issues),
            "overall_health_score": max(0, 100 - (system_data["cpu_usage"] + system_data["memory_usage"]) / 2)
        }

    def main():
        """Main monitoring loop"""
        print("üöÄ Starting Expert LLM Live Data Monitor...")
        
        while True:
            try:
                # Generate live data
                data = generate_live_system_data()
                
                # Save to file for the Expert LLM System to read
                with open('/app/data/live_monitoring.json', 'w') as f:
                    json.dump(data, f, indent=2)
                
                # Log key metrics
                metrics = data["system_metrics"]
                print(f"‚è∞ {metrics['timestamp'][:19]} | "
                      f"CPU: {metrics['cpu_usage']:.1f}% | "
                      f"MEM: {metrics['memory_usage']:.1f}% | "
                      f"Issues: {len(data['active_issues'])} | "
                      f"Health: {data['overall_health_score']:.0f}%")
                
                # Wait before next update
                time.sleep(30)  # Update every 30 seconds
                
            except Exception as e:
                print(f"‚ùå Error in monitoring loop: {e}")
                time.sleep(60)  # Wait longer on error

    if __name__ == "__main__":
        main()

  expert_patterns_live.yaml: |
    # Live Expert Patterns for Real-time Learning
    patterns:
      live_monitoring:
        name: "Live System Monitoring"
        category: "Real-time"
        description: "Real-time system health monitoring and alerting"
        pattern_type: "continuous"
        confidence_threshold: 0.7
        
        detection_rules:
          - type: "metric_threshold"
            metric: "cpu_usage"
            threshold: 80
            action: "alert_high_cpu"
          - type: "metric_threshold"
            metric: "memory_usage" 
            threshold: 85
            action: "alert_memory_pressure"
          - type: "pattern_match"
            patterns: ["CrashLoopBackOff", "ImagePullBackOff"]
            action: "k8s_pod_remediation"
        
        remediation_steps:
          alert_high_cpu:
            - "Identify top CPU consuming processes"
            - "Check for runaway processes or infinite loops"
            - "Consider scaling resources or optimizing code"
          alert_memory_pressure:
            - "Analyze memory usage patterns"
            - "Look for memory leaks in applications"
            - "Consider adding swap or increasing memory limits"
          k8s_pod_remediation:
            - "Check pod logs for error details"
            - "Verify image availability and configuration"
            - "Review resource limits and requests"

---
apiVersion: apps/v1
kind: Deployment
metadata:
  name: live-data-monitor
  namespace: expert-llm-system
  labels:
    app: live-data-monitor
    component: monitoring
spec:
  replicas: 1
  selector:
    matchLabels:
      app: live-data-monitor
  template:
    metadata:
      labels:
        app: live-data-monitor
        component: monitoring
    spec:
      containers:
      - name: data-monitor
        image: python:3.11-slim
        command: ["/bin/bash"]
        args:
        - -c
        - |
          pip install psutil requests &&
          mkdir -p /app/data &&
          python /config/monitoring_script.py
        volumeMounts:
        - name: config-volume
          mountPath: /config
        - name: data-volume
          mountPath: /app/data
        resources:
          requests:
            memory: "128Mi"
            cpu: "100m"
          limits:
            memory: "256Mi"
            cpu: "200m"
      volumes:
      - name: config-volume
        configMap:
          name: live-data-config
          defaultMode: 0755
      - name: data-volume
        emptyDir: {}
        
---
apiVersion: v1
kind: Service
metadata:
  name: live-data-service
  namespace: expert-llm-system
  labels:
    app: live-data-monitor
spec:
  type: ClusterIP
  ports:
  - port: 8080
    targetPort: 8080
    protocol: TCP
    name: http
  selector:
    app: live-data-monitor
